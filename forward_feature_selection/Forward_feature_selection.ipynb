{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import operator\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "root_path=current_path.replace('\\\\forward_feature_selection','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df=pd.read_csv(root_path+\"\\\\molecules.csv\",sep=\"\\t\"); mixed_df.head()\n",
    "f_classif_df=pd.read_csv(root_path+\"\\\\f_classif\\\\f_classif_best.csv\",sep=\"\\t\")\n",
    "mic_df=pd.read_csv(root_path+\"\\\\mutual_info_classif\\\\mic_best.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_disk(df,name:str,separator=\"\\t\"):\n",
    "    df.to_csv(name,sep=separator,index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_true_prediction(df,not_wanted_features:list):\n",
    "        temp_df=df.drop(not_wanted_features,axis=1)\n",
    "        y=temp_df[temp_df.columns[-1]]\n",
    "        x=temp_df.drop([temp_df.columns[-1]],axis=1)\n",
    "        \n",
    "        return x,y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_name_and_prediction(df,true_prediction,big_df):\n",
    "    new_df=df\n",
    "    new_df.insert(0,\"m_name\",big_df[\"m_name\"].values)\n",
    "    new_df=new_df.join(true_prediction)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecesary_features=[\"m_name\"]\n",
    "x,y=get_data_and_true_prediction(mixed_df,unnecesary_features)\n",
    "f_classif_x,f_classif_y=get_data_and_true_prediction(f_classif_df,unnecesary_features)\n",
    "mic_x,mic_y=get_data_and_true_prediction(mic_df,unnecesary_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc_score(x,y,model): # gets roc auc average\n",
    "        cv_results = cross_validate(model, x, y, cv=10,scoring=('roc_auc'))\n",
    "        roc_auc_avrg=cv_results['test_score'].mean()\n",
    "        \n",
    "        return roc_auc_avrg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0, tol=1e-5, dual=False) # model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can take any of the results, model will be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(x,y,model):\n",
    "    \n",
    "    def first_iteration(x,y,model):\n",
    "        score_lst=[]\n",
    "        for i in range(len(x.columns)):\n",
    "            k=x.columns[i]\n",
    "            temp_x=x[[k]]\n",
    "            score=get_roc_auc_score(temp_x,y,model)\n",
    "            score_lst.append(score)\n",
    "\n",
    "        max_score = max(score_lst) # best score\n",
    "        max_score_index=[i for i, j in enumerate(score_lst) if j == max_score] # indx with best score \n",
    "        new_features=[x.columns[i] for i in max_score_index]\n",
    "        top_new_features=new_features[0]\n",
    "        best_x=x[top_new_features]\n",
    "        new_x=x.drop(top_new_features,axis=1)\n",
    "        \n",
    "        return best_x,new_x,max_score\n",
    "    \n",
    "    def else_iteration(best_x,x,y,model,actual_score):\n",
    "        score_lst=[]\n",
    "        for i in range(len(x.columns)):\n",
    "            k=x.columns[i]\n",
    "            temp_x=x[[k]]\n",
    "            temp_new_x=pd.concat([best_x,temp_x],axis=1, ignore_index=True)\n",
    "            score=get_roc_auc_score(temp_new_x,y,model)\n",
    "            score_lst.append(score)\n",
    "            \n",
    "        max_score = max(score_lst) # best score        \n",
    "        \n",
    "        if(max_score<actual_score): return best_x,actual_score\n",
    "        \n",
    "        max_score_index=[i for i, j in enumerate(score_lst) if j == max_score] # indx with best score \n",
    "        \n",
    "        new_features=[x.columns[i] for i in max_score_index]\n",
    "        top_new_features=new_features[0]\n",
    "        temp_x=x[top_new_features]\n",
    "        best_x=pd.concat([best_x,temp_x],axis=1)\n",
    "        new_x=x.drop(top_new_features,axis=1)\n",
    "        \n",
    "        return else_iteration(best_x,new_x,y,model,max_score)\n",
    "    \n",
    "    best_x,new_x,score=first_iteration(x,y,model)\n",
    "    best_x,score=else_iteration(best_x,new_x,y,model,score)\n",
    "    \n",
    "    return best_x,score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1,score=forward_selection(x,y,clf); df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score: 89.05732547528203%\n"
     ]
    }
   ],
   "source": [
    "print(\"Model score: {}%\".format(score*100)) #Model score: 89.057650657384%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2,score=forward_selection(f_classif_x,f_classif_y,clf); df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score: 88.00086044216506%\n"
     ]
    }
   ],
   "source": [
    "print(\"Model score: {}%\".format(score*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 19)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3,score=forward_selection(mic_x,mic_y,clf); df_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score: 88.9372971483078%\n"
     ]
    }
   ],
   "source": [
    "print(\"Model score: {}%\".format(score*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIC and Normal seem the same, lest see their differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_best_columns=df_1.columns.values\n",
    "mic_best_columns=df_3.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_1= list(set(normal_best_columns)-set(mic_best_columns))\n",
    "diff_2= list(set(mic_best_columns)-set(normal_best_columns))\n",
    "same=list(set(normal_best_columns)&set(mic_best_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal has the next features, but mic_best doesn't ['n_saturated_carbocycles', 'n_aromatic_heterocycles', 'n_primary_carbon_atoms', 'm_avg_weigth']\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal has the next features, but mic_best doesn't {}\".format(diff_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mic_best has the next features, but normal doesn't ['n_atoms_without_Hydrogen', 'n_valence_electrons', 'n_aromatic_carbocycles', 'n_radical_electrons']\n"
     ]
    }
   ],
   "source": [
    "print(\"mic_best has the next features, but normal doesn't {}\".format(diff_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal and mic_best have in common: ['n_rings', 'm_logp', 'n_HBD', 'n_saturated_rings', 'n_O', 'n_non_strict_rotable_bonds', 'n_HOH', 'n_aliphatic_heterocycles', 'n_Hydrogen_donnors', 'n_atoms_with_Hydrogen', 'n_aromatic_rings', 'n_saturated_heterocycles', 'n_amide_bonds', 'n_briged_head_atoms', 'n_strict_rotable_bonds']\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal and mic_best have in common: {}\".format(same))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df = get_df_with_name_and_prediction(df_1,y,mixed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_disk(best_df,\"best_df_linear.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
