{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "root_path=current_path.replace('\\\\forward_feature_selection','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df=pd.read_csv(root_path+\"\\\\molecules.csv\",sep=\"\\t\")\n",
    "f_classif_df=pd.read_csv(root_path+\"\\\\f_classif\\\\f_classif_best.csv\",sep=\"\\t\")\n",
    "mic_df=pd.read_csv(root_path+\"\\\\mutual_info_classif\\\\mic_best.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_disk(df,name:str,separator=\"\\t\"):\n",
    "    df.to_csv(name,sep=separator,index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_true_prediction(df,not_wanted_features:list):\n",
    "        temp_df=df.drop(not_wanted_features,axis=1)\n",
    "        y=temp_df[temp_df.columns[-1]]\n",
    "        x=temp_df.drop([temp_df.columns[-1]],axis=1)\n",
    "        \n",
    "        return x,y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_name_and_prediction(df,true_prediction,big_df):\n",
    "    new_df=df\n",
    "    new_df.insert(0,\"m_name\",big_df[\"m_name\"].values)\n",
    "    new_df=new_df.join(true_prediction)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(df_lst:list,score_lst:list,switch:dict,true_prediction,big_df,new_df_name:str):\n",
    "\n",
    "    best_score = max(score_lst) # best score\n",
    "    best_score_index=[i for i, j in enumerate(score_lst) if j == best_score] # indx with best score \n",
    "    top_score_index=best_score_index[0]\n",
    "\n",
    "    print(switch[top_score_index]+\" is the best\")\n",
    "    print(\"score: {}%\".format(score_lst[top_score_index]*100))\n",
    "    best_df = get_df_with_name_and_prediction(df_lst[top_score_index],true_prediction,big_df)\n",
    "    \n",
    "    print(\"\\nFEATURES\\n\")\n",
    "    try:\n",
    "        for c,i in enumerate(best_df.columns):\n",
    "            print(\"{}. {}\".format(c+1,i))\n",
    "        save_df_to_disk(best_df,new_df_name)\n",
    "    except:\n",
    "        print(\"Data frame doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecesary_features=[\"m_name\"]\n",
    "x,y=get_data_and_true_prediction(mixed_df,unnecesary_features)\n",
    "f_classif_x,f_classif_y=get_data_and_true_prediction(f_classif_df,unnecesary_features)\n",
    "mic_x,mic_y=get_data_and_true_prediction(mic_df,unnecesary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types={0:'normal',1:'f_classif',2:'mic'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc_score(x,y,model): # gets roc auc average\n",
    "        cv_results = cross_validate(model, x, y, cv=10,scoring=('roc_auc'))\n",
    "        roc_auc_avrg=cv_results['test_score'].mean()\n",
    "        \n",
    "        return roc_auc_avrg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0, tol=1e-5, dual=False) # linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can take any of the results, model will be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(x,y,model): # O(n) worst case scenario, where n depends on len(x.columns)\n",
    "    \n",
    "    def first_iteration(x,y,model):\n",
    "        score_lst=[]\n",
    "        for i in range(len(x.columns)):\n",
    "            k=x.columns[i]\n",
    "            temp_x=x[[k]]\n",
    "            score=get_roc_auc_score(temp_x,y,model)\n",
    "            score_lst.append(score)\n",
    "\n",
    "        max_score = max(score_lst) # best score\n",
    "        max_score_index=[i for i, j in enumerate(score_lst) if j == max_score] # indx with best score \n",
    "        new_features=[x.columns[i] for i in max_score_index]\n",
    "        top_new_features=new_features[0]\n",
    "        best_x=x[top_new_features]\n",
    "        new_x=x.drop(top_new_features,axis=1)\n",
    "        \n",
    "        return best_x,new_x,max_score\n",
    "    \n",
    "    def else_iteration(best_x,x,y,model,actual_score):     \n",
    "        new_x_lenght = len(x.columns)\n",
    "        if (new_x_lenght > 0):\n",
    "            score_lst=[]\n",
    "            for i in range(new_x_lenght):\n",
    "                k=x.columns[i]\n",
    "                temp_x=x[[k]]\n",
    "                temp_new_x=pd.concat([best_x,temp_x],axis=1, ignore_index=True)\n",
    "                score=get_roc_auc_score(temp_new_x,y,model)\n",
    "                score_lst.append(score)\n",
    "\n",
    "            new_max_score = max(score_lst) # best score\n",
    "            actual_best_score = actual_score # score passed from parameters\n",
    "\n",
    "            if(new_max_score<actual_best_score):\n",
    "                return best_x,actual_best_score # break condition, recursive function\n",
    "\n",
    "            max_score_index=[i for i, j in enumerate(score_lst) if j == new_max_score] # indx with best score \n",
    "\n",
    "            new_features=[x.columns[i] for i in max_score_index]\n",
    "            top_new_features=new_features[0]\n",
    "            temp_x=x[top_new_features]\n",
    "            best_x=pd.concat([best_x,temp_x],axis=1)\n",
    "            new_x=x.drop(top_new_features,axis=1)\n",
    "\n",
    "            return else_iteration(best_x,new_x,y,model,new_max_score)\n",
    "        \n",
    "        return best_x,actual_score\n",
    "    \n",
    "    best_x,new_x,score=first_iteration(x,y,model)\n",
    "    best_x,score=else_iteration(best_x,new_x,y,model,score)\n",
    "    \n",
    "    return best_x,score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:24'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_1,score_normal=forward_selection(x,y,clf)\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:11'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_2,score_f_classif=forward_selection(f_classif_x,f_classif_y,clf)\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:17'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_3,score_mic=forward_selection(mic_x,mic_y,clf)\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal is the best\n",
      "score: 89.04773002246337%\n",
      "\n",
      "FEATURES\n",
      "\n",
      "1. m_name\n",
      "2. n_HBD\n",
      "3. n_O\n",
      "4. n_aliphatic_heterocycles\n",
      "5. m_logp\n",
      "6. n_aromatic_rings\n",
      "7. n_amide_bonds\n",
      "8. n_briged_head_atoms\n",
      "9. n_strict_rotable_bonds\n",
      "10. n_saturated_rings\n",
      "11. n_rings\n",
      "12. n_HOH\n",
      "13. n_non_strict_rotable_bonds\n",
      "14. n_atoms_with_Hydrogen\n",
      "15. n_primary_carbon_atoms\n",
      "16. n_aromatic_heterocycles\n",
      "17. n_saturated_heterocycles\n",
      "18. m_avg_weigth\n",
      "19. n_saturated_carbocycles\n",
      "20. n_aliphatic_carbocycles\n",
      "21. is_cns_molecule\n"
     ]
    }
   ],
   "source": [
    "save_best_model([df_1,df_2,df_3],[score_normal,score_f_classif,score_mic],df_types,y,mixed_df,\"best_classifier_linear.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_forward_selection_K_and_P_changes(x,y,k:list,pw:list,mt,alfa=6):\n",
    "    \n",
    "    def get_best_score_index(score_lst):\n",
    "        max_score = max(score_lst) # best score\n",
    "        max_score_index=[i for i, j in enumerate(score_lst) if j == max_score] # indx with best score \n",
    "        \n",
    "        return max_score_index[0]\n",
    "    \n",
    "    def inner_iteration(x,y,k,pw:list,mt):\n",
    "        print(\"k is now: {}\".format(k))\n",
    "        df_score_lst = []\n",
    "        for i in pw:\n",
    "            KNB_clf = KNeighborsClassifier(n_neighbors=k,p=i,metric=mt) # KNN model\n",
    "            df,score = forward_selection(x,y,KNB_clf)\n",
    "            df_score_lst.append([df,score])\n",
    "            \n",
    "        score_lst = [y for [x,y] in df_score_lst]\n",
    "        best_index = get_best_score_index(score_lst)\n",
    "        print(\"Best inner model when p = {}\".format(pw[best_index]))\n",
    "        return df_score_lst[best_index][0],df_score_lst[best_index][1],pw[best_index]\n",
    "        \n",
    "    print(f\"Metric: {mt}\\n\")\n",
    "    \n",
    "    outer_df_score_lst = []\n",
    "    temp_best_score = 0\n",
    "    \n",
    "    tries = int((len(k)/alfa)) # spare tries if by some point the performance decreases\n",
    "    \n",
    "    for cnt,i in enumerate(k):\n",
    "        best_inner_df,best_inner_score,best_p_value = inner_iteration(x,y,i,pw,mt)\n",
    "        print(\"Spare tries: {}\".format(tries))\n",
    "        \n",
    "        if((best_inner_score > temp_best_score) and tries > 0):\n",
    "            temp_best_score = best_inner_score\n",
    "            outer_df_score_lst.append([best_inner_df,best_inner_score,i,best_p_value])\n",
    "            if (cnt > 0): print(\"This iteration had an improvement\\n\")\n",
    "            elif (cnt == 0): print(\"\")             \n",
    "            tries = int((len(k)/alfa))               \n",
    "        else:\n",
    "            print(\"This iteration didn't have an improvement\\n\")\n",
    "            tries = tries-1\n",
    "        \n",
    "        if (tries == 0):\n",
    "            break\n",
    "                                              \n",
    "    outer_score_lst = [b for [a,b,c,d] in outer_df_score_lst]\n",
    "    best_outer_index = get_best_score_index(outer_score_lst)\n",
    "    \n",
    "    print(\"Final results\")\n",
    "    print(\"Best model when k = {} ,p = {} ,roc_auc = {}%\".format(outer_df_score_lst[best_outer_index][2],\n",
    "                                                                 outer_df_score_lst[best_outer_index][3],\n",
    "                                                                 outer_df_score_lst[best_outer_index][1]*100))\n",
    "    \n",
    "    return outer_df_score_lst[best_outer_index][0],outer_df_score_lst[best_outer_index][1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 50\n",
    "max_p = 10\n",
    "\n",
    "k_lst = [i for i in range(1,max_k+1)]\n",
    "p_lst = [i for i in range(1,max_p+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = 1, Manhattan Distance\n",
    "p = 2, Euclidean Distance\n",
    "p = ∞, Chebychev Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: minkowski\n",
      "\n",
      "k is now: 1\n",
      "Best inner model when p = 3\n",
      "Spare tries: 8\n",
      "\n",
      "k is now: 2\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 3\n",
      "Best inner model when p = 3\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 4\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 5\n",
      "Best inner model when p = 3\n",
      "Spare tries: 7\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 6\n",
      "Best inner model when p = 4\n",
      "Spare tries: 8\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 7\n",
      "Best inner model when p = 2\n",
      "Spare tries: 7\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 8\n",
      "Best inner model when p = 6\n",
      "Spare tries: 6\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 9\n",
      "Best inner model when p = 10\n",
      "Spare tries: 5\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 10\n",
      "Best inner model when p = 9\n",
      "Spare tries: 4\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 11\n",
      "Best inner model when p = 3\n",
      "Spare tries: 8\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 12\n",
      "Best inner model when p = 3\n",
      "Spare tries: 7\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 13\n",
      "Best inner model when p = 2\n",
      "Spare tries: 6\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 14\n",
      "Best inner model when p = 3\n",
      "Spare tries: 5\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 15\n",
      "Best inner model when p = 2\n",
      "Spare tries: 4\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 16\n",
      "Best inner model when p = 9\n",
      "Spare tries: 3\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 17\n",
      "Best inner model when p = 4\n",
      "Spare tries: 2\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 18\n",
      "Best inner model when p = 6\n",
      "Spare tries: 1\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "Final results\n",
      "Best model when k = 10 ,p = 9 ,roc_auc = 89.87858448747173%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'01:06:40'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_1,score_normal=KNN_forward_selection_K_and_P_changes(x,y,k_lst,p_lst,'minkowski')\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: minkowski\n",
      "\n",
      "k is now: 1\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "\n",
      "k is now: 2\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 3\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 4\n",
      "Best inner model when p = 7\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 5\n",
      "Best inner model when p = 3\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 6\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 7\n",
      "Best inner model when p = 4\n",
      "Spare tries: 7\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 8\n",
      "Best inner model when p = 10\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 9\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 10\n",
      "Best inner model when p = 5\n",
      "Spare tries: 7\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 11\n",
      "Best inner model when p = 4\n",
      "Spare tries: 8\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 12\n",
      "Best inner model when p = 3\n",
      "Spare tries: 7\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 13\n",
      "Best inner model when p = 2\n",
      "Spare tries: 6\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 14\n",
      "Best inner model when p = 4\n",
      "Spare tries: 5\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 15\n",
      "Best inner model when p = 2\n",
      "Spare tries: 4\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 16\n",
      "Best inner model when p = 2\n",
      "Spare tries: 3\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 17\n",
      "Best inner model when p = 2\n",
      "Spare tries: 2\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 18\n",
      "Best inner model when p = 2\n",
      "Spare tries: 1\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "Final results\n",
      "Best model when k = 10 ,p = 5 ,roc_auc = 89.04360846836134%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'00:35:49'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_2,score_f_classif=KNN_forward_selection_K_and_P_changes(f_classif_x,f_classif_y,k_lst,p_lst,'minkowski')\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: minkowski\n",
      "\n",
      "k is now: 1\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "\n",
      "k is now: 2\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 3\n",
      "Best inner model when p = 2\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 4\n",
      "Best inner model when p = 1\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 5\n",
      "Best inner model when p = 8\n",
      "Spare tries: 8\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 6\n",
      "Best inner model when p = 10\n",
      "Spare tries: 7\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 7\n",
      "Best inner model when p = 4\n",
      "Spare tries: 6\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 8\n",
      "Best inner model when p = 7\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 9\n",
      "Best inner model when p = 5\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 10\n",
      "Best inner model when p = 5\n",
      "Spare tries: 8\n",
      "This iteration had an improvement\n",
      "\n",
      "k is now: 11\n",
      "Best inner model when p = 2\n",
      "Spare tries: 8\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 12\n",
      "Best inner model when p = 3\n",
      "Spare tries: 7\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 13\n",
      "Best inner model when p = 3\n",
      "Spare tries: 6\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 14\n",
      "Best inner model when p = 2\n",
      "Spare tries: 5\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 15\n",
      "Best inner model when p = 1\n",
      "Spare tries: 4\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 16\n",
      "Best inner model when p = 1\n",
      "Spare tries: 3\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 17\n",
      "Best inner model when p = 1\n",
      "Spare tries: 2\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "k is now: 18\n",
      "Best inner model when p = 1\n",
      "Spare tries: 1\n",
      "This iteration didn't have an improvement\n",
      "\n",
      "Final results\n",
      "Best model when k = 10 ,p = 5 ,roc_auc = 89.38857248154206%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'00:48:01'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_3,score_mic=KNN_forward_selection_K_and_P_changes(mic_x,mic_y,k_lst,p_lst,'minkowski')\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best KNN “minkowski”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal is the best\n",
      "score: 89.87858448747173%\n",
      "\n",
      "FEATURES\n",
      "\n",
      "1. m_name\n",
      "2. n_hetero_atoms\n",
      "3. n_aliphatic_heterocycles\n",
      "4. n_O\n",
      "5. n_HBD\n",
      "6. n_non_strict_rotable_bonds\n",
      "7. n_atoms_stereo_centers\n",
      "8. n_hetero_cycles\n",
      "9. n_amide_bonds\n",
      "10. n_briged_head_atoms\n",
      "11. n_aromatic_carbocycles\n",
      "12. fraction_CSP3\n",
      "13. is_cns_molecule\n"
     ]
    }
   ],
   "source": [
    "save_best_model([df_1,df_2,df_3],[score_normal,score_f_classif,score_mic],df_types,y,mixed_df,\"best_classifier_knn_minkowski.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=10, p=9 for this knn model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
