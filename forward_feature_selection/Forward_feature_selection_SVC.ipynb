{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "root_path=current_path.replace('\\\\forward_feature_selection','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df=pd.read_csv(root_path+\"\\\\molecules.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_disk(df,name:str,separator=\"\\t\"):\n",
    "    df.to_csv(name,sep=separator,index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_true_prediction(df,not_wanted_features:list):\n",
    "        temp_df=df.drop(not_wanted_features,axis=1)\n",
    "        y=temp_df[temp_df.columns[-1]]\n",
    "        x=temp_df.drop([temp_df.columns[-1]],axis=1)\n",
    "        \n",
    "        return x,y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_name_and_prediction(df,true_prediction,big_df):\n",
    "    new_df=df\n",
    "    new_df.insert(0,\"m_name\",big_df[\"m_name\"].values)\n",
    "    new_df=new_df.join(true_prediction)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc_score(x,y,model): # gets roc auc average\n",
    "        cv_results = cross_validate(model, x, y, cv=10,scoring=('roc_auc'))\n",
    "        roc_auc_avrg=cv_results['test_score'].mean()\n",
    "        \n",
    "        return roc_auc_avrg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_score_index(score_lst:list):\n",
    "    max_score = max(score_lst) # best score\n",
    "    max_score_index=[i for i, j in enumerate(score_lst) if j == max_score] # indx with best score \n",
    "        \n",
    "    return max_score_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(x,y,model): # O(n) worst case scenario, where n depends on len(x.columns)\n",
    "    \n",
    "    def first_iteration(x,y,model):\n",
    "        score_lst = []\n",
    "        for i in range(len(x.columns)):\n",
    "            k = x.columns[i]\n",
    "            temp_x = x[[k]]\n",
    "            score = get_roc_auc_score(temp_x,y,model)\n",
    "            score_lst.append(score)\n",
    "    \n",
    "        max_score = max(score_lst) # best score\n",
    "        max_score_index = [i for i, j in enumerate(score_lst) if j == max_score] # indx with best score\n",
    "        top_score_index = max_score_index[0]\n",
    "        new_feature = x.columns[top_score_index] \n",
    "        best_x = x[new_feature]\n",
    "        new_x = x.drop(new_feature,axis=1)\n",
    "        \n",
    "        return best_x,new_x,max_score\n",
    "    \n",
    "    def else_iteration(best_x,x,y,model,actual_score):     \n",
    "        new_x_lenght = len(x.columns)\n",
    "        if (new_x_lenght > 0):\n",
    "            score_lst = []\n",
    "            for i in range(new_x_lenght):\n",
    "                k = x.columns[i]\n",
    "                temp_x = x[[k]]\n",
    "                temp_new_x = pd.concat([best_x,temp_x],axis=1, ignore_index=True)\n",
    "                score = get_roc_auc_score(temp_new_x,y,model)\n",
    "                score_lst.append(score)\n",
    "\n",
    "            max_score = max(score_lst) # best score\n",
    "\n",
    "            if(max_score<actual_score):\n",
    "                return best_x,actual_score # break condition, recursive function\n",
    "\n",
    "            max_score_index = [i for i, j in enumerate(score_lst) if j == max_score] # indx with best score\n",
    "            top_score_index = max_score_index[0]\n",
    "\n",
    "            new_feature = x.columns[top_score_index]\n",
    "            temp_x = x[new_feature]\n",
    "            best_x = pd.concat([best_x,temp_x],axis=1)\n",
    "            new_x = x.drop(new_feature,axis=1)\n",
    "\n",
    "            return else_iteration(best_x,new_x,y,model,max_score)\n",
    "        \n",
    "        return best_x,actual_score\n",
    "    \n",
    "    f_best_x,new_x,f_score = first_iteration(x,y,model)\n",
    "    best_x,best_score = else_iteration(f_best_x,new_x,y,model,f_score)\n",
    "    \n",
    "    return best_x,best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def forward_partition_SVC(x,y,lst:list,other): # list must be greater than len 2    \n",
    "    print(\"\\nsecond param list len is {}\".format(len(lst)))\n",
    "    print(\"second param list -> {}\".format(lst))\n",
    "    \n",
    "    if (len(lst) == 2):\n",
    "        first = lst[0]\n",
    "        print(\"first is {}\".format(first))\n",
    "        last = lst[1]\n",
    "        print(\"last is {}\".format(last))\n",
    "        clf_first = SVC(gamma=other, C=first,random_state=0) \n",
    "        first_df,first_score = forward_selection(x,y,clf_first)\n",
    "        clf_last = SVC(gamma=other, C=last,random_state=0)\n",
    "        last_df,last_score = forward_selection(x,y,clf_last)\n",
    "        \n",
    "        if (last_score > first_score):\n",
    "            print(\"last score is greater than first score\")\n",
    "            return last_df,last,last_score\n",
    "        \n",
    "        print(\"first score is greater than last score\")\n",
    "        return first_df,first,first_score\n",
    "    \n",
    "    elif(len(lst) == 3):\n",
    "        first = lst[0]\n",
    "        print(\"first is {}\".format(first))\n",
    "        middle = lst[1]\n",
    "        print(\"middle is {}\".format(middle))\n",
    "        last = lst[-1]\n",
    "        print(\"last is {}\".format(last))\n",
    "        clf_first = SVC(gamma=other, C=first,random_state=0)\n",
    "        _,first_score = forward_selection(x,y,clf_first)       \n",
    "        clf_middle = SVC(gamma=other, C=middle,random_state=0)\n",
    "        _,middle_score = forward_selection(x,y,clf_middle)\n",
    "        clf_last = SVC(gamma=other, C=last,random_state=0)\n",
    "        _,last_score = forward_selection(x,y,clf_last)\n",
    "        \n",
    "        if ((first_score >= middle_score) and (last_score <= middle_score)):\n",
    "            print(\"sub list taken: left\")\n",
    "            return forward_partition_SVC(x,y,lst[:2],other)  \n",
    "        \n",
    "        print(\"sub list taken: right\")            \n",
    "        return forward_partition_SVC(x,y,lst[1:],other)\n",
    "    \n",
    "    else:          \n",
    "        first = lst[0]\n",
    "        print(\"first is {}\".format(first))\n",
    "        middle = lst[(len(lst)//2)-1]\n",
    "        print(\"middle is {}\".format(middle))\n",
    "        last = lst[-1]\n",
    "        print(\"last is {}\".format(last))\n",
    "        clf_first = SVC(gamma=other, C=first,random_state=0)\n",
    "        _,first_score = forward_selection(x,y,clf_first)       \n",
    "        clf_middle = SVC(gamma=other, C=middle,random_state=0)\n",
    "        _,middle_score = forward_selection(x,y,clf_middle)\n",
    "        clf_last = SVC(gamma=other, C=last,random_state=0)\n",
    "        _,last_score = forward_selection(x,y,clf_last)\n",
    "        \n",
    "        if ((first_score >= middle_score) and (last_score <= middle_score)):\n",
    "            print(\"sub list taken: left\")\n",
    "            return forward_partition_SVC(x,y,lst[:(len(lst)//2)],other)\n",
    "        \n",
    "        print(\"sub list taken: right\")            \n",
    "        return forward_partition_SVC(x,y,lst[(len(lst)//2)-1:],other)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def best_SVC(x,y,K:list,P:list,alfa=6):\n",
    "    best_prm_lst = []\n",
    "    temp_score = 0\n",
    "    spare_tries = math.floor(len(K)/alfa)+1\n",
    "    \n",
    "    for i in K:\n",
    "        print(\"----------------------------\\n\") \n",
    "        print(\"First parameter is {}\".format(i))\n",
    "        best_df,best_p,best_score = forward_partition_SVC(x,y,P,i)  \n",
    "                  \n",
    "        print(\"\\n----------------------------\")    \n",
    "        print(\"When first parameter is {}\".format(i))\n",
    "        print(\"Best seconds parameter is {}\".format(best_p))\n",
    "        print(\"ROC AUC = {}\".format(best_score))\n",
    "        print(\"\\nFeatures\\n\")\n",
    "        for c,i in enumerate(best_df.columns):\n",
    "            print(\"{}. {}\".format(c+1,i))\n",
    "            \n",
    "        if (temp_score < best_score):\n",
    "            temp_score = best_score\n",
    "            spare_tries = math.floor(len(K)/alfa)+1\n",
    "        else:\n",
    "            print(\"\\nResults didn't improve in last iteration\")\n",
    "            spare_tries=spare_tries-1\n",
    "        \n",
    "        if(spare_tries <= 0): break      \n",
    "        best_prm_lst.append([best_df,i,best_p,best_score])\n",
    "        \n",
    "    score_lst = [d for [a,b,c,d] in best_prm_lst]\n",
    "           \n",
    "    print(\"\\n------------ Final results ----------------\")\n",
    "    index = get_best_score_index(score_lst)\n",
    "    best_prm = best_prm_lst[index]\n",
    "    print(\"Best model when first param = {} ,second param = {} ,ROC AUC = {}\".format(best_prm[1],\n",
    "                                                                 best_prm[2],\n",
    "                                                                 best_prm[-1]))\n",
    "    return best_prm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guarantee_results(x,y,K:list,P:list):\n",
    "    best_outer_prm_lst = []\n",
    "    for i in K:\n",
    "        print(\"First param is {}\\n\".format(i))\n",
    "        best_inner_prm_lst = []\n",
    "        for j in P:\n",
    "            print(\"Second param is {}\".format(j))\n",
    "            clf = SVC(random_state=0, C=i,gamma=j) # linear model\n",
    "            best_df,best_score = forward_selection(x,y,clf)\n",
    "            print(\"ROC AUC = {}\".format(best_score))\n",
    "            print(\"\\nFeatures\\n\")\n",
    "            for c,d in enumerate(best_df.columns):\n",
    "                print(\"{}. {}\".format(c+1,d))       \n",
    "            print(\"\")\n",
    "            best_inner_prm_lst.append([best_df,i,j,best_score])\n",
    "            \n",
    "        score_lst = [d  for [a,b,c,d] in best_inner_prm_lst]\n",
    "        index = get_best_score_index(score_lst)\n",
    "        best_outer_prm_lst.append([best_inner_prm_lst[index][0],best_inner_prm_lst[index][1],best_inner_prm_lst[index][2],best_inner_prm_lst[index][3]])\n",
    "        \n",
    "    score_lst = [d  for [a,b,c,d] in best_outer_prm_lst]\n",
    "    index = get_best_score_index(score_lst)\n",
    "    print(\"Final results\")\n",
    "    print(\"Best model when first param = {} ,second param = {} ,ROC AUC = {}\".format(best_outer_prm_lst[index][1],\n",
    "                                                                 best_outer_prm_lst[index][2],\n",
    "                                                                 best_outer_prm_lst[index][3]))\n",
    "    return best_outer_prm_lst[index][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecesary_features=[\"m_name\"]\n",
    "x,y = get_data_and_true_prediction(mixed_df,unnecesary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "Gamma = [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "best_df = best_SVC(x,y,C,Gamma)\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "g_df = guarantee_results(x,y,C,Gamma)\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=get_df_with_name_and_prediction(g_df,y,mixed_df)\n",
    "save_df_to_disk(new_df,\"best_classifier_SVC.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C = 17\n",
    "Gamma = 0.01\n",
    "Tol = 0.001\n",
    "ROC AUC = 0.9022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
