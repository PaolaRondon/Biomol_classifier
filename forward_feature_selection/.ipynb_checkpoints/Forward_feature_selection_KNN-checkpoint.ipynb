{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "root_path=current_path.replace('\\\\forward_feature_selection','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df=pd.read_csv(root_path+\"\\\\molecules.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_disk(df,name:str,separator=\"\\t\"):\n",
    "    df.to_csv(name,sep=separator,index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_true_prediction(df,not_wanted_features:list):\n",
    "        temp_df=df.drop(not_wanted_features,axis=1)\n",
    "        y=temp_df[temp_df.columns[-1]]\n",
    "        x=temp_df.drop([temp_df.columns[-1]],axis=1)\n",
    "        \n",
    "        return x,y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_name_and_prediction(df,true_prediction,big_df):\n",
    "    new_df=df\n",
    "    new_df.insert(0,\"m_name\",big_df[\"m_name\"].values)\n",
    "    new_df=new_df.join(true_prediction)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc_score(x,y,model): # gets roc auc average\n",
    "        cv_results = cross_validate(model, x, y, cv=10,scoring=('roc_auc'))\n",
    "        roc_auc_avrg=cv_results['test_score'].mean()\n",
    "        \n",
    "        return roc_auc_avrg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_forward_selection(x,y,k:list,pw:list,mt,alfa=6):\n",
    "    \n",
    "    def get_best_score_index(score_lst):\n",
    "        max_score = max(score_lst) # best score\n",
    "        max_score_index=[i for i, j in enumerate(score_lst) if j == max_score] # indx with best score \n",
    "        \n",
    "        return max_score_index[0]\n",
    "    \n",
    "    def inner_iteration(x,y,k,pw:list,mt):\n",
    "        print(\"k is now: {}\".format(k))\n",
    "        df_score_lst = []\n",
    "        for i in pw:\n",
    "            KNB_clf = KNeighborsClassifier(n_neighbors=k,p=i,metric=mt) # KNN model\n",
    "            df,score = forward_selection(x,y,KNB_clf)\n",
    "            df_score_lst.append([df,score])\n",
    "            \n",
    "        score_lst = [y for [x,y] in df_score_lst]\n",
    "        best_index = get_best_score_index(score_lst)\n",
    "        print(\"Best inner model when p = {}\".format(pw[best_index]))\n",
    "        print(\"\\nFeatures\\n\")\n",
    "        for c,i in enumerate(df_score_lst[best_index][0].columns):\n",
    "            print(\"{}. {}\".format(c+1,i))\n",
    "        \n",
    "        return df_score_lst[best_index][0],df_score_lst[best_index][1],pw[best_index]\n",
    "        \n",
    "    print(f\"Metric: {mt}\\n\")\n",
    "    \n",
    "    outer_df_score_lst = []\n",
    "    temp_best_score = 0\n",
    "    \n",
    "    tries = math.ceil((len(k)/alfa))+1 # spare tries if by some point the performance decreases\n",
    "    \n",
    "    for cnt,i in enumerate(k):\n",
    "        best_inner_df,best_inner_score,best_p_value = inner_iteration(x,y,i,pw,mt)\n",
    "        print(\"\\nSpare tries: {}\".format(tries))\n",
    "        \n",
    "        if((best_inner_score > temp_best_score) and tries > 0):\n",
    "            temp_best_score = best_inner_score\n",
    "            outer_df_score_lst.append([best_inner_df,best_inner_score,i,best_p_value])\n",
    "            if (cnt > 0): print(\"This iteration had an improvement\\n\")\n",
    "            elif (cnt == 0): print(\"\")             \n",
    "            tries = math.ceil((len(k)/alfa))+1            \n",
    "        else:\n",
    "            print(\"This iteration didn't have an improvement\\n\")\n",
    "            tries = tries-1\n",
    "        \n",
    "        if (tries == 0):\n",
    "            break\n",
    "                                              \n",
    "    outer_score_lst = [b for [a,b,c,d] in outer_df_score_lst]\n",
    "    best_outer_index = get_best_score_index(outer_score_lst)\n",
    "    \n",
    "    print(\"Final results\")\n",
    "    print(\"Best model when k = {} ,p = {} ,roc_auc = {}%\".format(outer_df_score_lst[best_outer_index][2],\n",
    "                                                                 outer_df_score_lst[best_outer_index][3],\n",
    "                                                                 outer_df_score_lst[best_outer_index][1]*100))\n",
    "    \n",
    "    return outer_df_score_lst[best_outer_index][0],outer_df_score_lst[best_outer_index][1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecesary_features=[\"m_name\"]\n",
    "x,y=get_data_and_true_prediction(mixed_df,unnecesary_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 50\n",
    "max_p = 15\n",
    "\n",
    "k_lst = [i for i in range(1,max_k+1)]\n",
    "p_lst = [i for i in range(1,max_p+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = 1, Manhattan Distance\n",
    "p = 2, Euclidean Distance\n",
    "p = âˆž, Chebychev Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df,score_normal=KNN_forward_selection(x,y,k_lst,p_lst,'minkowski')\n",
    "end = time.time()\n",
    "\n",
    "time.strftime('%H:%M:%S', time.gmtime(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
