{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "root_path=current_path.replace('\\\\forward_feature_selection','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df=pd.read_csv(root_path+\"\\\\molecules.csv\",sep=\"\\t\")\n",
    "f_classif_df=pd.read_csv(root_path+\"\\\\f_classif\\\\f_classif_best.csv\",sep=\"\\t\")\n",
    "mic_df=pd.read_csv(root_path+\"\\\\mutual_info_classif\\\\mic_best.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_disk(df,name:str,separator=\"\\t\"):\n",
    "    df.to_csv(name,sep=separator,index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_true_prediction(df,not_wanted_features:list):\n",
    "        temp_df=df.drop(not_wanted_features,axis=1)\n",
    "        y=temp_df[temp_df.columns[-1]]\n",
    "        x=temp_df.drop([temp_df.columns[-1]],axis=1)\n",
    "        \n",
    "        return x,y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_name_and_prediction(df,true_prediction,big_df):\n",
    "    new_df=df\n",
    "    new_df.insert(0,\"m_name\",big_df[\"m_name\"].values)\n",
    "    new_df=new_df.join(true_prediction)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecesary_features=[\"m_name\"]\n",
    "x,y=get_data_and_true_prediction(mixed_df,unnecesary_features)\n",
    "f_classif_x,f_classif_y=get_data_and_true_prediction(f_classif_df,unnecesary_features)\n",
    "mic_x,mic_y=get_data_and_true_prediction(mic_df,unnecesary_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc_score(x,y,model): # gets roc auc average\n",
    "        cv_results = cross_validate(model, x, y, cv=10,scoring=('roc_auc'))\n",
    "        roc_auc_avrg=cv_results['test_score'].mean()\n",
    "        \n",
    "        return roc_auc_avrg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0, tol=1e-5, dual=False) # model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can take any of the results, model will be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(x,y,model): # O(n) worst case scenario, where n depends on len(x.columns)\n",
    "    \n",
    "    def first_iteration(x,y,model):\n",
    "        score_lst=[]\n",
    "        for i in range(len(x.columns)):\n",
    "            k=x.columns[i]\n",
    "            temp_x=x[[k]]\n",
    "            score=get_roc_auc_score(temp_x,y,model)\n",
    "            score_lst.append(score)\n",
    "\n",
    "        max_score = max(score_lst) # best score\n",
    "        max_score_index=[i for i, j in enumerate(score_lst) if j == max_score] # indx with best score \n",
    "        new_features=[x.columns[i] for i in max_score_index]\n",
    "        top_new_features=new_features[0]\n",
    "        best_x=x[top_new_features]\n",
    "        new_x=x.drop(top_new_features,axis=1)\n",
    "        \n",
    "        return best_x,new_x,max_score\n",
    "    \n",
    "    def else_iteration(best_x,x,y,model,actual_score):     \n",
    "        new_x_lenght = len(x.columns)\n",
    "        if (new_x_lenght > 0):\n",
    "            score_lst=[]\n",
    "            for i in range(new_x_lenght):\n",
    "                k=x.columns[i]\n",
    "                temp_x=x[[k]]\n",
    "                temp_new_x=pd.concat([best_x,temp_x],axis=1, ignore_index=True)\n",
    "                score=get_roc_auc_score(temp_new_x,y,model)\n",
    "                score_lst.append(score)\n",
    "\n",
    "            new_max_score = max(score_lst) # best score\n",
    "            actual_best_score = actual_score # score passed from parameters\n",
    "\n",
    "            if(new_max_score<actual_best_score):\n",
    "                return best_x,actual_best_score # break condition, recursive function\n",
    "\n",
    "            max_score_index=[i for i, j in enumerate(score_lst) if j == new_max_score] # indx with best score \n",
    "\n",
    "            new_features=[x.columns[i] for i in max_score_index]\n",
    "            top_new_features=new_features[0]\n",
    "            temp_x=x[top_new_features]\n",
    "            best_x=pd.concat([best_x,temp_x],axis=1)\n",
    "            new_x=x.drop(top_new_features,axis=1)\n",
    "\n",
    "            return else_iteration(best_x,new_x,y,model,new_max_score)\n",
    "        \n",
    "        return best_x,actual_score\n",
    "    \n",
    "    best_x,new_x,score=first_iteration(x,y,model)\n",
    "    best_x,score=else_iteration(best_x,new_x,y,model,score)\n",
    "    \n",
    "    return best_x,score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 19)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1,score_normal=forward_selection(x,y,clf); df_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2,score_f_classif=forward_selection(f_classif_x,f_classif_y,clf); df_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 18)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3,score_mic=forward_selection(mic_x,mic_y,clf); df_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIC and Normal seem the same, lest see their differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_best_columns=df_1.columns.values\n",
    "mic_best_columns=df_3.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_1= list(set(normal_best_columns)-set(mic_best_columns))\n",
    "diff_2= list(set(mic_best_columns)-set(normal_best_columns))\n",
    "same=list(set(normal_best_columns)&set(mic_best_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal has the next features, but mic_best doesn't ['n_atoms_with_Hydrogen', 'n_aromatic_heterocycles', 'n_saturated_rings', 'n_rings', 'n_primary_carbon_atoms', 'm_avg_weigth']\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal has the next features, but mic_best doesn't {}\".format(diff_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mic_best has the next features, but normal doesn't ['n_atoms_stereo_centers', 'n_atoms_without_Hydrogen', 'n_hetero_cycles', 'n_atoms_unspecified_stereo_centers', 'n_valence_electrons']\n"
     ]
    }
   ],
   "source": [
    "print(\"mic_best has the next features, but normal doesn't {}\".format(diff_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal and mic_best have in common: ['m_logp', 'n_amide_bonds', 'n_aromatic_rings', 'n_HOH', 'n_aliphatic_heterocycles', 'n_aliphatic_carbocycles', 'n_O', 'n_non_strict_rotable_bonds', 'n_strict_rotable_bonds', 'n_saturated_heterocycles', 'n_briged_head_atoms', 'n_saturated_carbocycles', 'n_HBD']\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal and mic_best have in common: {}\".format(same))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal is the best\n",
      "score: 89.04773002246337%\n"
     ]
    }
   ],
   "source": [
    "df_lst= [df_1,df_2,df_3]\n",
    "score_lst = [score_normal,score_f_classif,score_mic]\n",
    "switch={0:'normal',1:'f_classif',2:'mic'}\n",
    "\n",
    "general_df=mixed_df\n",
    "general_y=y\n",
    "\n",
    "best_score = max(score_lst) # best score\n",
    "best_score_index=[i for i, j in enumerate(score_lst) if j == best_score] # indx with best score \n",
    "top_score_index=best_score_index[0]\n",
    "\n",
    "print(switch[top_score_index]+\" is the best\")\n",
    "print(\"score: {}%\".format(score_lst[top_score_index]*100))\n",
    "best_df = get_df_with_name_and_prediction(df_lst[top_score_index],general_y,general_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. m_name\n",
      "2. n_HBD\n",
      "3. n_O\n",
      "4. n_aliphatic_heterocycles\n",
      "5. m_logp\n",
      "6. n_aromatic_rings\n",
      "7. n_amide_bonds\n",
      "8. n_briged_head_atoms\n",
      "9. n_strict_rotable_bonds\n",
      "10. n_saturated_rings\n",
      "11. n_rings\n",
      "12. n_HOH\n",
      "13. n_non_strict_rotable_bonds\n",
      "14. n_atoms_with_Hydrogen\n",
      "15. n_primary_carbon_atoms\n",
      "16. n_aromatic_heterocycles\n",
      "17. n_saturated_heterocycles\n",
      "18. m_avg_weigth\n",
      "19. n_saturated_carbocycles\n",
      "20. n_aliphatic_carbocycles\n",
      "21. is_cns_molecule\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for c,i in enumerate(best_df.columns):\n",
    "        print(\"{}. {}\".format(c+1,i))\n",
    "    save_df_to_disk(best_df,\"best_df_linear.csv\")\n",
    "except:\n",
    "    print(\"Data frame doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_forward_selection(x,y,k,pw,mt):\n",
    "    \n",
    "    def get_best_score_index(score_lst):\n",
    "        max_score = max(score_lst) # best score\n",
    "        max_score_index=[i for i, j in enumerate(score_lst) if j == max_score] # indx with best score \n",
    "        top_score_index=max_score_index[0]\n",
    "        \n",
    "        return top_score_index\n",
    "    \n",
    "    df_score_lst=[]\n",
    "    for i in range(1,k+1):\n",
    "        KNB_clf=KNeighborsClassifier(n_neighbors=i,p=pw,metric=mt)\n",
    "        df,score = forward_selection(x,y,KNB_clf)\n",
    "        df_score_lst.append([df,score])\n",
    "        \n",
    "    score_lst=[y for [x,y] in df_score_lst]\n",
    "    index=get_best_score_index(score_lst)\n",
    "    \n",
    "    return df_score_lst[index][0],df_score_lst[index][1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1,score_nomal=KNN_forward_selection(x,y,20,2,'minkowski')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2,score_f_classif=KNN_forward_selection(f_classif_x,f_classif_y,20,2,'minkowski')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3,score_mic=KNN_forward_selection(mic_x,mic_y,20,2,'minkowski')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mic is the best\n",
      "score: 89.52035252836825%\n"
     ]
    }
   ],
   "source": [
    "df_lst= [df_1,df_2,df_3]\n",
    "score_lst = [score_normal,score_f_classif,score_mic]\n",
    "switch={0:'normal',1:'f_classif',2:'mic'}\n",
    "\n",
    "general_df=mixed_df\n",
    "general_y=y\n",
    "\n",
    "best_score = max(score_lst) # best score\n",
    "best_score_index=[i for i, j in enumerate(score_lst) if j == best_score] # indx with best score \n",
    "top_score_index=best_score_index[0]\n",
    "\n",
    "print(switch[top_score_index]+\" is the best\")\n",
    "print(\"score: {}%\".format(score_lst[top_score_index]*100))\n",
    "best_df = get_df_with_name_and_prediction(df_lst[top_score_index],general_y,general_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. m_name\n",
      "2. n_HBD\n",
      "3. n_hetero_atoms\n",
      "4. n_aliphatic_heterocycles\n",
      "5. n_non_strict_rotable_bonds\n",
      "6. n_amide_bonds\n",
      "7. n_atoms_stereo_centers\n",
      "8. n_briged_head_atoms\n",
      "9. n_hetero_cycles\n",
      "10. n_O\n",
      "11. n_aromatic_carbocycles\n",
      "12. n_saturated_carbocycles\n",
      "13. is_cns_molecule\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for c,i in enumerate(best_df.columns):\n",
    "        print(\"{}. {}\".format(c+1,i))\n",
    "    save_df_to_disk(best_df,\"best_df_KNN.csv\")\n",
    "except:\n",
    "    print(\"Data frame doesn't exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
